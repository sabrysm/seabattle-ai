{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6nlkKdqZghw7C/6qX8eSM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabrysm/seabattle-ai/blob/master/Sea_Battle_Training_Phase_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]\n",
        "!pip install dm-reverb[tensorflow]\n",
        "!pip install tensorflow-probability\n",
        "!pip install Pillow\n",
        "!pip install protobuf"
      ],
      "metadata": {
        "id": "XN8U3FQAcDFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21360b2-2518-4245-f2de-21dd0ecbb90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-agents[reverb]\n",
            "  Downloading tf_agents-0.18.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents[reverb])\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf-agents[reverb])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-probability~=0.22.0 (from tf-agents[reverb])\n",
            "  Downloading tensorflow_probability-0.22.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.13.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.13.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow~=2.14.0 (from tf-agents[reverb])\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.13.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (16.0.6)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow~=2.14.0->tf-agents[reverb])\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (2.3.0)\n",
            "Collecting wrapt>=1.11.1 (from tf-agents[reverb])\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-agents[reverb]) (1.59.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow~=2.14.0->tf-agents[reverb])\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow~=2.14.0->tf-agents[reverb])\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow~=2.14.0->tf-agents[reverb])\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf-agents[reverb]) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.13.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-agents[reverb]) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697631 sha256=4199c6b6183cd6fd59c6f973435a38c09b20caf8d2d94b53abb9729e94dd0c3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: wrapt, tensorflow-probability, tensorflow-estimator, rlds, pygame, ml-dtypes, keras, gym, tf-agents, dm-reverb, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.20.1\n",
            "    Uninstalling tensorflow-probability-0.20.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.20.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.1\n",
            "    Uninstalling ml-dtypes-0.3.1:\n",
            "      Successfully uninstalled ml-dtypes-0.3.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed dm-reverb-0.13.0 gym-0.23.0 keras-2.14.0 ml-dtypes-0.2.0 pygame-2.1.3 rlds-0.1.8 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-probability-0.22.0 tf-agents-0.18.0 wrapt-1.14.1\n",
            "Requirement already satisfied: dm-reverb[tensorflow] in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb[tensorflow]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb[tensorflow]) (1.5.2)\n",
            "Requirement already satisfied: tensorflow~=2.14.0 in /usr/local/lib/python3.10/dist-packages (from dm-reverb[tensorflow]) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.14.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb[tensorflow]) (5.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.23.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.1.8)\n",
            "Requirement already satisfied: typing-extensions<4.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (4.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKdkyPMaJTSa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from typing import Sequence\n",
        "from absl import app\n",
        "import reverb\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tf_agents as tfa\n",
        "from tf_agents.agents.reinforce import reinforce_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import tf_py_environment, py_environment\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.specs import tensor_spec, array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.utils import common\n",
        "import time\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "BOARD_SIZE = 8\n",
        "ITERATIONS = 250000\n",
        "COLLECT_EPISODES_PER_ITERATION = 1\n",
        "REPLAY_BUFFER_CAPACITY = 2000\n",
        "REPLAY_BUFFER_TABLE_NAME = \"uniform_table\"\n",
        "DISCOUNT = 0.995\n",
        "\n",
        "FC_LAYER_PARAMS = 64\n",
        "\n",
        "LEARNING_RATE = 1e-2\n",
        "NUM_EVAL_EPISODES = 20\n",
        "EVAL_INTERVAL = 500\n",
        "CHECKPOINT_INTERVAL = 2000\n",
        "DOWNLOAD_INTERVAL = 10530\n",
        "\n",
        "LOGDIR = \"./tf_agents_log_bs7\"\n",
        "MODELDIR = \"./\"\n",
        "POLICYDIR = \"./bs7_downloads\"\n",
        "train_dir = './bs7_training'\n",
        "NAMING_CONVENTION = 'bs7_exported_cp_'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SHIPS_SIZE = np.array([2 ,2 ,3, 3, 4])\n",
        "MAX_STEPS_PER_EPISODE = BOARD_SIZE**2\n",
        "\n",
        "# Rewards for each strike\n",
        "HIT_REWARD = 1\n",
        "MISS_REWARD = 0\n",
        "REPEAT_STRIKE_REWARD = -1\n",
        "REPEAT_MISS_REWARD = -1\n",
        "# Reward for finishing the game within MAX_STEPS_PER_EPISODE\n",
        "FINISHED_GAME_REWARD = 10\n",
        "# Reward for not finishing the game within MAX_STEPS_PER_EPISODE\n",
        "UNFINISHED_GAME_REWARD = -10\n",
        "\n",
        "# Hidden board cell status; 'occupied' means it's part of the plane\n",
        "HIDDEN_BOARD_CELL_OCCUPIED = 1\n",
        "HIDDEN_BOARD_CELL_UNOCCUPIED = 0\n",
        "\n",
        "# Visible board cell status\n",
        "VISIBLE_BOARD_CELL_HIT = 1\n",
        "VISIBLE_BOARD_CELL_MISS = -1\n",
        "VISIBLE_BOARD_CELL_DESTROYED = 2\n",
        "VISIBLE_BOARD_CELL_UNTRIED = 0\n",
        "\n",
        "\n",
        "all_ships = {\n",
        "    1:2,\n",
        "    2:2,\n",
        "    3:3,\n",
        "    4:3,\n",
        "    5:4\n",
        "}\n",
        "\n",
        "destroyed_ships = {\n",
        "    1:0,\n",
        "    2:0,\n",
        "    3:0,\n",
        "    4:0,\n",
        "    5:0\n",
        "}\n",
        "\n",
        "\n",
        "class BattleshipEnv(py_environment.PyEnvironment):\n",
        "    def __init__(self,\n",
        "               board_size=BOARD_SIZE,\n",
        "               ships_size=SHIPS_SIZE,\n",
        "               discount=0.9,\n",
        "               max_steps=MAX_STEPS_PER_EPISODE) -> None:\n",
        "        assert board_size >= 4\n",
        "        self._board_size = board_size\n",
        "        self._strike_count = 0\n",
        "        self._discount = discount\n",
        "        self._max_steps = max_steps\n",
        "        self._episode_ended = False\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "          (), np.int32, minimum=0, maximum=self._board_size**2 - 1)\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "          (self._board_size, self._board_size),\n",
        "          np.float32,\n",
        "          minimum=VISIBLE_BOARD_CELL_MISS,\n",
        "          maximum=VISIBLE_BOARD_CELL_HIT)\n",
        "        self._time_step_spec = ts.time_step_spec(self._observation_spec)\n",
        "        self.set_boards()\n",
        "\n",
        "    def set_boards(self):\n",
        "        self._ships_size = SHIPS_SIZE\n",
        "        self._hit_count = 0\n",
        "        self._visible_board = np.zeros((self._board_size, self._board_size))\n",
        "        self._hidden_board = self.place_ships(self._board_size, self._ships_size)\n",
        "\n",
        "    def current_time_step(self):\n",
        "        return self._current_time_step\n",
        "\n",
        "    def observation_spec(self):\n",
        "        \"\"\"Return observation_spec.\"\"\"\n",
        "        return self._observation_spec\n",
        "\n",
        "    def action_spec(self):\n",
        "        \"\"\"Return action_spec.\"\"\"\n",
        "        return self._action_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Return initial_time_step.\"\"\"\n",
        "        self._episode_ended = False\n",
        "        self._strike_count = 0\n",
        "        self._hit_count = 0\n",
        "        self.set_boards()\n",
        "        return ts.restart(np.array(self._visible_board, dtype=np.float32))\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"Apply action and return new time_step.\"\"\"\n",
        "        if self._hit_count == np.sum(self._ships_size):\n",
        "            self._episode_ended = True\n",
        "            return self.reset()\n",
        "\n",
        "        if self._strike_count + 1 == self._max_steps:\n",
        "            self.reset()\n",
        "            return ts.termination(\n",
        "            np.array(self._visible_board, dtype=np.float32),\n",
        "            UNFINISHED_GAME_REWARD)\n",
        "\n",
        "        self._strike_count += 1\n",
        "        action_x = action // self._board_size\n",
        "        action_y = action % self._board_size\n",
        "        # Hit\n",
        "        if self._hidden_board[action_x][action_y] >= 1:\n",
        "            # Non-repeat move\n",
        "            if self._visible_board[action_x][action_y] == VISIBLE_BOARD_CELL_UNTRIED:\n",
        "                additional_reward = 0\n",
        "                self._hit_count += 1\n",
        "                self._visible_board[action_x][action_y] = VISIBLE_BOARD_CELL_HIT\n",
        "                # increment number of hits\n",
        "                destroyed_ships[self._hidden_board[action_x][action_y]] += 1\n",
        "                positions = np.where(self._hidden_board[action_x][action_y] == 6) # continue here\n",
        "                if destroyed_ships[self._hidden_board[action_x][action_y]] == all_ships[self._hidden_board[action_x][action_y]]:\n",
        "                    # the ship is destroyed\n",
        "                    positions = np.where(self._hidden_board == self._hidden_board[action_x][action_y])\n",
        "                    self._visible_board[positions] = VISIBLE_BOARD_CELL_DESTROYED # means it is destroyed\n",
        "                    additional_reward = 4\n",
        "                # Successful strike\n",
        "                if self._hit_count == np.sum(self._ships_size):\n",
        "                    # Game finished\n",
        "                    self._episode_ended = True\n",
        "                    return ts.termination(np.array(self._visible_board, dtype=np.float32),FINISHED_GAME_REWARD)\n",
        "                else:\n",
        "                    self._episode_ended = False\n",
        "                    return ts.transition(\n",
        "                        np.array(self._visible_board, dtype=np.float32), HIT_REWARD + additional_reward,\n",
        "                        self._discount)\n",
        "            # Repeat strike\n",
        "            else:\n",
        "                self._episode_ended = False\n",
        "                return ts.transition(\n",
        "                      np.array(self._visible_board, dtype=np.float32),\n",
        "                      REPEAT_STRIKE_REWARD, self._discount)\n",
        "        # Miss\n",
        "        else:\n",
        "            if self._visible_board[action_x][action_y] == VISIBLE_BOARD_CELL_UNTRIED:\n",
        "                # Unsuccessful strike\n",
        "                self._episode_ended = False\n",
        "                self._visible_board[action_x][action_y] = VISIBLE_BOARD_CELL_MISS\n",
        "                return ts.transition(\n",
        "                    np.array(self._visible_board, dtype=np.float32), MISS_REWARD,\n",
        "                    self._discount)\n",
        "            else:\n",
        "                self._episode_ended = False\n",
        "                return ts.transition(\n",
        "                    np.array(self._visible_board, dtype=np.float32), REPEAT_STRIKE_REWARD,\n",
        "                    self._discount)\n",
        "\n",
        "    def place_ships(self, grid_size, ships_length):\n",
        "        ships_location = np.zeros((grid_size, grid_size))\n",
        "        ind = 0\n",
        "        for ship_length in ships_length:\n",
        "            # Randomly place ship horizontally or vertically without overlapping\n",
        "            if np.random.randint(0, 2) == 0: # Horizontal\n",
        "                start_row = np.random.randint(0, grid_size)\n",
        "                start_col = np.random.randint(0, grid_size - ship_length + 1)\n",
        "                while np.sum(ships_location[start_row, start_col:start_col+ship_length]) > 0:\n",
        "                    start_row = np.random.randint(0, grid_size)\n",
        "                    start_col = np.random.randint(0, grid_size - ship_length + 1)\n",
        "                ships_location[start_row, start_col:start_col+ship_length] = ind + 1\n",
        "            else: # Vertical\n",
        "                start_row = np.random.randint(0, grid_size - ship_length + 1)\n",
        "                start_col = np.random.randint(0, grid_size)\n",
        "                while np.sum(ships_location[start_row:start_row+ship_length, start_col]) > 0:\n",
        "                    start_col = np.random.randint(0, grid_size)\n",
        "                    start_row = np.random.randint(0, grid_size - ship_length + 1)\n",
        "                ships_location[start_row:start_row+ship_length, start_col] = ind + 1\n",
        "            ind += 1\n",
        "        return ships_location"
      ],
      "metadata": {
        "id": "4lZsa7KXLouw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_avg_return_and_steps(environment, policy, num_episodes=10):\n",
        "    \"\"\"Compute average return and # of steps.\"\"\"\n",
        "    total_return = 0.0\n",
        "    total_steps = 0.0\n",
        "    total_repeated_steps = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_return = 0.0\n",
        "        episode_steps = 0.0\n",
        "        episode_repeated_steps = 0.0\n",
        "        actions = []\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step)\n",
        "            if action_step.action.numpy()[0] in actions:\n",
        "                episode_repeated_steps += 1\n",
        "            else:\n",
        "                actions.append(action_step.action.numpy()[0])\n",
        "            time_step = environment.step(action_step.action)\n",
        "            episode_return += time_step.reward\n",
        "            episode_steps += 1\n",
        "        total_return += episode_return\n",
        "        total_steps += episode_steps\n",
        "        total_repeated_steps += episode_repeated_steps\n",
        "    average_return = total_return / num_episodes\n",
        "    average_episode_steps = total_steps / num_episodes\n",
        "    average_repeated_steps = total_repeated_steps / num_episodes\n",
        "    return average_return.numpy()[0], average_episode_steps, average_repeated_steps"
      ],
      "metadata": {
        "id": "20l6OqYtdY0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_episode(environment, policy, num_episodes, replay_buffer_observer):\n",
        "    \"\"\"Collect game episode trajectories.\"\"\"\n",
        "    initial_time_step = environment.reset()\n",
        "\n",
        "    driver = py_driver.PyDriver(\n",
        "        environment,\n",
        "        py_tf_eager_policy.PyTFEagerPolicy(policy, use_tf_function=True),\n",
        "        [replay_buffer_observer],\n",
        "        max_episodes=num_episodes,\n",
        "    )\n",
        "    initial_time_step = environment.reset()\n",
        "    driver.run(initial_time_step)"
      ],
      "metadata": {
        "id": "K48w0Fl9dqJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_zip_file(dirname, base_filename):\n",
        "  return shutil.make_archive(base_filename, 'zip', dirname)"
      ],
      "metadata": {
        "id": "89xTJN5yI8o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(iterations, modeldir, logdir, policydir):\n",
        "    \"\"\"Train and convert the model using TF Agents.\"\"\"\n",
        "\n",
        "    train_py_env = BattleshipEnv(\n",
        "        board_size=BOARD_SIZE, discount=DISCOUNT, max_steps=BOARD_SIZE**2\n",
        "    )\n",
        "    eval_py_env = BattleshipEnv(\n",
        "        board_size=BOARD_SIZE, discount=DISCOUNT, max_steps=BOARD_SIZE**2\n",
        "    )\n",
        "\n",
        "    train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "    # Alternatively you could use ActorDistributionNetwork as actor_net\n",
        "    actor_net = tfa.networks.Sequential(\n",
        "        [\n",
        "            tfa.keras_layers.InnerReshape([BOARD_SIZE, BOARD_SIZE], [BOARD_SIZE**2]),\n",
        "            tf.keras.layers.Dense(3*FC_LAYER_PARAMS, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(BOARD_SIZE**2),\n",
        "            tf.keras.layers.Lambda(lambda t: tfp.distributions.Categorical(logits=t)),\n",
        "        ],\n",
        "        input_spec=train_py_env.observation_spec(),\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "\n",
        "    tf_agent = reinforce_agent.ReinforceAgent(\n",
        "        train_env.time_step_spec(),\n",
        "        train_env.action_spec(),\n",
        "        actor_network=actor_net,\n",
        "        optimizer=optimizer,\n",
        "        normalize_returns=True,\n",
        "        train_step_counter=global_step,\n",
        "    )\n",
        "\n",
        "    tf_agent.initialize()\n",
        "\n",
        "    eval_policy = tf_agent.policy\n",
        "    collect_policy = tf_agent.collect_policy\n",
        "\n",
        "    tf_policy_saver = policy_saver.PolicySaver(collect_policy)\n",
        "\n",
        "    # Use reverb as replay buffer\n",
        "    replay_buffer_signature = tensor_spec.from_spec(tf_agent.collect_data_spec)\n",
        "    replay_buffer_signature = tensor_spec.add_outer_dim(replay_buffer_signature)\n",
        "    table = reverb.Table(\n",
        "        REPLAY_BUFFER_TABLE_NAME,\n",
        "        max_size=REPLAY_BUFFER_CAPACITY,\n",
        "        sampler=reverb.selectors.Uniform(),\n",
        "        remover=reverb.selectors.Fifo(),\n",
        "        rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "        signature=replay_buffer_signature,\n",
        "    )  # specify signature here for validation at insertion time\n",
        "\n",
        "    reverb_server = reverb.Server([table])\n",
        "\n",
        "    replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "        tf_agent.collect_data_spec,\n",
        "        sequence_length=None,\n",
        "        table_name=REPLAY_BUFFER_TABLE_NAME,\n",
        "        local_server=reverb_server,\n",
        "    )\n",
        "\n",
        "    replay_buffer_observer = reverb_utils.ReverbAddEpisodeObserver(\n",
        "        replay_buffer.py_client, REPLAY_BUFFER_TABLE_NAME, REPLAY_BUFFER_CAPACITY\n",
        "    )\n",
        "    train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=train_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=tf_agent,\n",
        "    policy=tf_agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        "    )\n",
        "    policy_checkpointer = common.Checkpointer(\n",
        "      ckpt_dir=os.path.join(train_dir, 'policy'),\n",
        "      policy=eval_policy,\n",
        "      global_step=global_step,\n",
        "    )\n",
        "    rb_checkpointer = common.Checkpointer(\n",
        "        ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n",
        "        max_to_keep=1,\n",
        "        replay_buffer=replay_buffer,\n",
        "    )\n",
        "    train_checkpointer.initialize_or_restore()\n",
        "    rb_checkpointer.initialize_or_restore()\n",
        "    policy_checkpointer.initialize_or_restore()\n",
        "    global_step = tf.compat.v1.train.get_global_step()\n",
        "    # Optimize by wrapping some of the code in a graph using TF function.\n",
        "    tf_agent.train = common.function(tf_agent.train)\n",
        "\n",
        "    # Evaluate the agent's policy once before training.\n",
        "    avg_return = compute_avg_return_and_steps(\n",
        "        eval_env, tf_agent.policy, NUM_EVAL_EPISODES\n",
        "    )\n",
        "\n",
        "    summary_writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Collect a few episodes using collect_policy and save to the replay buffer.\n",
        "        collect_episode(\n",
        "            train_py_env,\n",
        "            collect_policy,\n",
        "            COLLECT_EPISODES_PER_ITERATION,\n",
        "            replay_buffer_observer,\n",
        "        )\n",
        "\n",
        "        # Use data from the buffer and update the agent's network.\n",
        "        iterator = iter(replay_buffer.as_dataset(sample_batch_size=1))\n",
        "        trajectories, _ = next(iterator)\n",
        "        tf_train = tf_agent.train(experience=trajectories)\n",
        "        replay_buffer.clear()\n",
        "\n",
        "        logger = tf.get_logger()\n",
        "        if i % CHECKPOINT_INTERVAL == 0:\n",
        "            train_checkpointer.save(global_step=global_step.numpy())\n",
        "            rb_checkpointer.save(global_step=global_step.numpy())\n",
        "            policy_checkpointer.save(global_step=global_step.numpy())\n",
        "            print(f\"\\nSaved Checkpoint: {global_step.numpy()}\")\n",
        "        if i % DOWNLOAD_INTERVAL == 0:\n",
        "            tf_policy_saver.save(policydir)\n",
        "            create_zip_file(train_dir, NAMING_CONVENTION + '%d' % global_step)\n",
        "            print(\"Downloaded the Policy\")\n",
        "        if i % EVAL_INTERVAL == 0:\n",
        "            avg_return, avg_episode_length, avg_repeated_steps = compute_avg_return_and_steps(\n",
        "                eval_env, eval_policy, NUM_EVAL_EPISODES\n",
        "            )\n",
        "            with summary_writer.as_default():\n",
        "                tf.summary.scalar(\"Average return\", avg_return, step=i)\n",
        "                tf.summary.scalar(\"Average episode length\", avg_episode_length, step=i)\n",
        "                summary_writer.flush()\n",
        "            logger.info(\n",
        "                \"iteration = {0}: Average Return = {1}, Average Episode Length = {2}\".format(\n",
        "                    i, avg_return, avg_episode_length\n",
        "                )\n",
        "            )\n",
        "            seconds = time.time()\n",
        "            local_time = time.ctime(seconds)\n",
        "            print(f\"Continuing from step {global_step.numpy()} at {local_time}\")\n",
        "            print(f\"Loss: {tf_train.loss}\")\n",
        "            print(\n",
        "                'iteration = {0}: avg Return = {1}, avg Episode Length = {2}, avg repeated actions = {3}'\n",
        "                .format(global_step.numpy(), avg_return, avg_episode_length, avg_repeated_steps))\n",
        "\n",
        "    summary_writer.close()\n",
        "\n",
        "    tf_policy_saver.save(policydir)"
      ],
      "metadata": {
        "id": "VNIGKYUWds_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_agent(ITERATIONS, MODELDIR, LOGDIR, POLICYDIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLiuuZ6cdzSg",
        "outputId": "2d41a506-b5c8-4325-8934-d5ea53132edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved Checkpoint: 720027\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Categorical_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded the Policy\n",
            "Continuing from step 720027 at Mon Oct 16 12:52:32 2023\n",
            "Loss: -2.3879926204681396\n",
            "iteration = 720027: avg Return = 6.300000190734863, avg Episode Length = 51.45, avg repeated actions = 6.7\n",
            "Continuing from step 720527 at Mon Oct 16 12:57:18 2023\n",
            "Loss: -0.5716915130615234\n",
            "iteration = 720527: avg Return = 2.5, avg Episode Length = 59.25, avg repeated actions = 6.2\n",
            "Continuing from step 721027 at Mon Oct 16 13:02:01 2023\n",
            "Loss: -5.934597969055176\n",
            "iteration = 721027: avg Return = 6.5, avg Episode Length = 54.55, avg repeated actions = 4.9\n",
            "Continuing from step 721527 at Mon Oct 16 13:06:38 2023\n",
            "Loss: 9.449164390563965\n",
            "iteration = 721527: avg Return = 9.850000381469727, avg Episode Length = 50.05, avg repeated actions = 4.45\n",
            "\n",
            "Saved Checkpoint: 722027\n",
            "Continuing from step 722027 at Mon Oct 16 13:11:19 2023\n",
            "Loss: 16.29084014892578\n",
            "iteration = 722027: avg Return = 3.3499999046325684, avg Episode Length = 53.65, avg repeated actions = 8.85\n",
            "Continuing from step 722527 at Mon Oct 16 13:15:40 2023\n",
            "Loss: 4.754644393920898\n",
            "iteration = 722527: avg Return = 13.800000190734863, avg Episode Length = 49.65, avg repeated actions = 2.55\n",
            "Continuing from step 723027 at Mon Oct 16 13:20:04 2023\n",
            "Loss: 2.2174813747406006\n",
            "iteration = 723027: avg Return = 7.400000095367432, avg Episode Length = 52.35, avg repeated actions = 5.05\n",
            "Continuing from step 723527 at Mon Oct 16 13:24:42 2023\n",
            "Loss: 1.25657320022583\n",
            "iteration = 723527: avg Return = 4.25, avg Episode Length = 53.75, avg repeated actions = 6.75\n",
            "\n",
            "Saved Checkpoint: 724027\n",
            "Continuing from step 724027 at Mon Oct 16 13:29:03 2023\n",
            "Loss: -0.1955394744873047\n",
            "iteration = 724027: avg Return = 9.25, avg Episode Length = 52.2, avg repeated actions = 4.1\n",
            "Continuing from step 724527 at Mon Oct 16 13:33:31 2023\n",
            "Loss: -5.152552604675293\n",
            "iteration = 724527: avg Return = 11.600000381469727, avg Episode Length = 44.9, avg repeated actions = 4.4\n",
            "Continuing from step 725027 at Mon Oct 16 13:37:57 2023\n",
            "Loss: -5.825117588043213\n",
            "iteration = 725027: avg Return = 11.149999618530273, avg Episode Length = 50.95, avg repeated actions = 4.15\n",
            "Continuing from step 725527 at Mon Oct 16 13:42:29 2023\n",
            "Loss: 11.651698112487793\n",
            "iteration = 725527: avg Return = 7.849999904632568, avg Episode Length = 52.35, avg repeated actions = 5.35\n",
            "\n",
            "Saved Checkpoint: 726027\n",
            "Continuing from step 726027 at Mon Oct 16 13:47:04 2023\n",
            "Loss: 11.071741104125977\n",
            "iteration = 726027: avg Return = 3.950000047683716, avg Episode Length = 53.95, avg repeated actions = 7.35\n",
            "Continuing from step 726527 at Mon Oct 16 13:51:38 2023\n",
            "Loss: 0.6076151728630066\n",
            "iteration = 726527: avg Return = 5.550000190734863, avg Episode Length = 53.95, avg repeated actions = 6.55\n",
            "Continuing from step 727027 at Mon Oct 16 13:56:16 2023\n",
            "Loss: 0.7526246905326843\n",
            "iteration = 727027: avg Return = 5.75, avg Episode Length = 54.05, avg repeated actions = 6.45\n",
            "Continuing from step 727527 at Mon Oct 16 14:00:53 2023\n",
            "Loss: -2.644815444946289\n",
            "iteration = 727527: avg Return = 11.149999618530273, avg Episode Length = 47.65, avg repeated actions = 4.0\n",
            "\n",
            "Saved Checkpoint: 728027\n",
            "Continuing from step 728027 at Mon Oct 16 14:05:28 2023\n",
            "Loss: -2.057046413421631\n",
            "iteration = 728027: avg Return = 11.5, avg Episode Length = 50.9, avg repeated actions = 3.75\n",
            "Continuing from step 728527 at Mon Oct 16 14:10:02 2023\n",
            "Loss: -7.131032466888428\n",
            "iteration = 728527: avg Return = 12.25, avg Episode Length = 51.7, avg repeated actions = 3.0\n",
            "Continuing from step 729027 at Mon Oct 16 14:14:36 2023\n",
            "Loss: -12.382326126098633\n",
            "iteration = 729027: avg Return = 6.699999809265137, avg Episode Length = 52.75, avg repeated actions = 6.5\n",
            "Continuing from step 729527 at Mon Oct 16 14:19:16 2023\n",
            "Loss: -5.263030529022217\n",
            "iteration = 729527: avg Return = 9.550000190734863, avg Episode Length = 54.35, avg repeated actions = 3.6\n",
            "\n",
            "Saved Checkpoint: 730027\n",
            "Continuing from step 730027 at Mon Oct 16 14:23:47 2023\n",
            "Loss: -5.251485347747803\n",
            "iteration = 730027: avg Return = 11.100000381469727, avg Episode Length = 51.2, avg repeated actions = 4.1\n",
            "Continuing from step 730527 at Mon Oct 16 14:28:25 2023\n",
            "Loss: -2.3288984298706055\n",
            "iteration = 730527: avg Return = 3.3499999046325684, avg Episode Length = 54.95, avg repeated actions = 6.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Categorical_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded the Policy\n",
            "Continuing from step 731027 at Mon Oct 16 14:33:04 2023\n",
            "Loss: -6.930057525634766\n",
            "iteration = 731027: avg Return = 11.850000381469727, avg Episode Length = 50.45, avg repeated actions = 3.35\n",
            "Continuing from step 731527 at Mon Oct 16 14:37:51 2023\n",
            "Loss: 1.4612560272216797\n",
            "iteration = 731527: avg Return = 11.949999809265137, avg Episode Length = 51.4, avg repeated actions = 3.45\n",
            "\n",
            "Saved Checkpoint: 732027\n",
            "Continuing from step 732027 at Mon Oct 16 14:42:30 2023\n",
            "Loss: -3.922785758972168\n",
            "iteration = 732027: avg Return = 14.399999618530273, avg Episode Length = 45.7, avg repeated actions = 2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Agent"
      ],
      "metadata": {
        "id": "Ybz0G9eQD2IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.policies import policy_loader\n",
        "\n",
        "\n",
        "# global_step = tf.compat.v1.train.get_global_step()\n",
        "# print(global_step.numpy())\n",
        "saved_policy = tf.saved_model.load(POLICYDIR)\n",
        "eval_py_env = BattleshipEnv(\n",
        "    board_size=BOARD_SIZE, discount=DISCOUNT, max_steps=BOARD_SIZE**2\n",
        ")\n",
        "\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "start = time.time()\n",
        "avg_return, avg_episode_length, avg_repeat = compute_avg_return_and_steps(\n",
        "          eval_env, saved_policy, 20)\n",
        "print('Average Return = {0}, Average Episode Length = {1}'.format(avg_return, avg_episode_length))\n",
        "end = time.time()\n",
        "\n",
        "print(f\"secs: {end-start} avg repeat: {avg_repeat}\")\n",
        "# saved_policy.get_train_step()\n",
        "# saved_policy.action()"
      ],
      "metadata": {
        "id": "6yTRBy6mD6Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hinRX314Gucj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}